{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1 style=\"text-align:center;font-size:80px;color:gold;\">Computer Vision Based Text Scanner</h1>\n\n<p style=\"font-size:30px;color:black;\">As most of the OCR based text extractors works only on structured typed documents and fails to detect words in an unstructured kind of documents. So in an addition to previously developed OCR tools we tried to add a functionality to detect even handwritten text documents. Obviously this tool is not a 100% accurate but we tried to maintain its accuracy between 70-80% as structure and formats of handwritten documents varied person to person.</p>","metadata":{}},{"cell_type":"markdown","source":"# Handwritten Text Recognition\n\nHandwritten Text recognition involves the automatic conversion of text in an image into letter codes that are usable within computer and text-processing applications. In simple terms, it is the text extraction from your handwritten notebooks/pages.","metadata":{}},{"cell_type":"markdown","source":"## Approach\n\n* **Step1** :  Build a digit(0-9) + A-Z characters classifier using a CNN architecture.\n* **Step2** :  Apply character segmentation for the handwritten word image.\n* **Step3** :  Classify each segmented letter and then get the final word in the image.","metadata":{}},{"cell_type":"code","source":"!pip install imutils","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom keras.preprocessing.image import ImageDataGenerator\nimport os\nimport random \nimport cv2\nimport imutils\nimport random\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelBinarizer\nfrom keras.utils import np_utils\nfrom keras.models import Sequential\nfrom keras import optimizers\nfrom sklearn.preprocessing import LabelBinarizer\nfrom keras import backend as K\nfrom keras.layers import Dense, Activation, Flatten, Dense,MaxPooling2D, Dropout\nfrom keras.layers import Conv2D, MaxPooling2D, BatchNormalization","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dir = \"../input/handwritten-characters/Train/\"\ntrain_data = []\nimg_size = 32\nnon_chars = [\"#\",\"$\",\"&\",\"@\"]\nfor i in os.listdir(dir):\n    if i in non_chars:\n        continue\n    count = 0\n    sub_directory = os.path.join(dir,i)\n    for j in os.listdir(sub_directory):\n        count+=1\n        if count > 4000:\n            break\n        img = cv2.imread(os.path.join(sub_directory,j),0)\n        img = cv2.resize(img,(img_size,img_size))\n        train_data.append([img,i])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_dir = \"../input/handwritten-characters/Validation/\"\nval_data = []\nimg_size = 32\nfor i in os.listdir(val_dir):\n    if i in non_chars:\n        continue\n    count = 0\n    sub_directory = os.path.join(val_dir,i)\n    for j in os.listdir(sub_directory):\n        count+=1\n        if count > 1000:\n            break\n        img = cv2.imread(os.path.join(sub_directory,j),0)\n        img = cv2.resize(img,(img_size,img_size))\n        val_data.append([img,i])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(val_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"random.shuffle(train_data)\nrandom.shuffle(val_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_X = []\ntrain_Y = []\nfor features,label in train_data:\n    train_X.append(features)\n    train_Y.append(label)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_X = []\nval_Y = []\nfor features,label in val_data:\n    val_X.append(features)\n    val_Y.append(label)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LB = LabelBinarizer()\ntrain_Y = LB.fit_transform(train_Y)\nval_Y = LB.fit_transform(val_Y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_X = np.array(train_X)/255.0\ntrain_X = train_X.reshape(-1,32,32,1)\ntrain_Y = np.array(train_Y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_X = np.array(val_X)/255.0\nval_X = val_X.reshape(-1,32,32,1)\nval_Y = np.array(val_Y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_X.shape,val_X.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_Y.shape,val_Y.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Sequential()\n\nmodel.add(Conv2D(32, (3, 3), padding = \"same\", activation='relu', input_shape=(32,32,1)))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Conv2D(128, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Dropout(0.25))\n \nmodel.add(Flatten())\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(35, activation='softmax'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(loss='categorical_crossentropy', optimizer=\"adam\",metrics=['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(train_X,train_Y, epochs=50, batch_size=32, validation_data = (val_X, val_Y),  verbose=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Training Accuracy vs Validation Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper left')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Training Loss vs Validation Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper left')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Recognition and Post-Processing \n1. The sort contours function is used to get the correct order of individual characters for correct output extraction. In this case for extracting a single word, a left to right sorting of individual characters is needed.\n2. The get letters function fetches the list of letters and get word function gets the individual word. ","metadata":{}},{"cell_type":"code","source":"def sort_contours(cnts, method=\"left-to-right\"):\n    reverse = False\n    i = 0\n    if method == \"right-to-left\" or method == \"bottom-to-top\":\n        reverse = True\n    if method == \"top-to-bottom\" or method == \"bottom-to-top\":\n        i = 1\n    boundingBoxes = [cv2.boundingRect(c) for c in cnts]\n    (cnts, boundingBoxes) = zip(*sorted(zip(cnts, boundingBoxes),\n    key=lambda b:b[1][i], reverse=reverse))\n    # return the list of sorted contours and bounding boxes\n    return (cnts, boundingBoxes)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_letters(img):\n    letters = []\n    image = cv2.imread(img)\n    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    ret,thresh1 = cv2.threshold(gray ,127,255,cv2.THRESH_BINARY_INV)\n    dilated = cv2.dilate(thresh1, None, iterations=2)\n\n    cnts = cv2.findContours(dilated.copy(), cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n    cnts = imutils.grab_contours(cnts)\n    cnts = sort_contours(cnts, method=\"left-to-right\")[0]\n    # loop over the contours\n    for c in cnts:\n        if cv2.contourArea(c) > 10:\n            (x, y, w, h) = cv2.boundingRect(c)\n            cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n        roi = gray[y:y + h, x:x + w]\n        thresh = cv2.threshold(roi, 0, 255,cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)[1]\n        thresh = cv2.resize(thresh, (32, 32), interpolation = cv2.INTER_CUBIC)\n        thresh = thresh.astype(\"float32\") / 255.0\n        thresh = np.expand_dims(thresh, axis=-1)\n        thresh = thresh.reshape(1,32,32,1)\n        ypred = model.predict(thresh)\n        ypred = LB.inverse_transform(ypred)\n        [x] = ypred\n        letters.append(x)\n    return letters, image\n\n#plt.imshow(image)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_word(letter):\n    word = \"\".join(letter)\n    return word","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"letter,image = get_letters(\"../input/handwriting-recognition/train_v2/train/TRAIN_00003.jpg\")\nword = get_word(letter)\nprint(word)\nplt.imshow(image)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"letter,image = get_letters(\"../input/handwriting-recognition/train_v2/train/TRAIN_00023.jpg\")\nword = get_word(letter)\nprint(word)\nplt.imshow(image)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"letter,image = get_letters(\"../input/handwriting-recognition/train_v2/train/TRAIN_00030.jpg\")\nword = get_word(letter)\nprint(word)\nplt.imshow(image)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"letter,image = get_letters(\"../input/handwriting-recognition/validation_v2/validation/VALIDATION_0005.jpg\")\nword = get_word(letter)\nprint(word)\nplt.imshow(image)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"letter,image = get_letters(\"../input/handwriting-recognition/test_v2/test/TEST_0007.jpg\")\nword = get_word(letter)\nprint(word)\nplt.imshow(image)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Drawbacks\n1. The recognition part is dependent on the contour detection code, so if the opencv library is not able to find the character contour, then this method will fail.\n2. There could be a lot of variation in a single handwritten letter in terms of writing style, therefore a lot more examples are needed for training this model.\n3. This model will not work for connected texts like a cursive handwritten word.","metadata":{}},{"cell_type":"markdown","source":"## Conclusion \nThis notebook is an illustration of how a character segmentation and classification approach can be used for offline handwritten text extraction. In order to improve the model, the model should be trained on the complete dataset, this notebook was trained on slightly less number of images due to session constraints. Also, for applying this method to a complete paragraph, following approach can be used, **line segmentation >> word segmentation >> character segmentation >> classification >> post-processing**. ","metadata":{}},{"cell_type":"markdown","source":"## References\n1. [https://www.pyimagesearch.com/2020/08/24/ocr-handwriting-recognition-with-opencv-keras-and-tensorflow/](http://) \n2. [https://www.pyimagesearch.com/2015/04/20/sorting-contours-using-python-and-opencv/](http://)","metadata":{}}]}